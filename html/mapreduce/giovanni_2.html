<html lang="en">

  <head>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>MapReduce</title>

    <!-- Bootstrap core CSS -->
    <link href="../../vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom fonts for this template -->
    <link href="../../vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Merriweather:400,300,300italic,400italic,700,700italic,900,900italic' rel='stylesheet' type='text/css'>

    <!-- Plugin CSS -->
    <!--link href="vendor/magnific-popup/magnific-popup.css" rel="stylesheet"-->

    <!-- Custom styles for this template -->
    <!--link href="css/creative.min.css" rel="stylesheet"-->
    <link href="../../css/creative.css" rel="stylesheet">
    <script defer src="https://use.fontawesome.com/releases/v5.0.8/js/all.js"></script>

  <style>

    .scrollspy-example {
  position: relative;
  height: 700px;
  overflow: scroll;
}
  </style>

  </head>

  <body id="page-top">

    <!-- Navigation -->
    <!-- The navbar expands the collapsed menu above the large (lg) breakpoint, in all other
    cases the menu will be -->
    <nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNavbar">
    <!--nav class="navbar navbar-toggleable-md navbar-light bg-faded" id="mainNavbar"-->
      <div class="container">
        <a class="navbar-brand js-scroll-trigger" href="#page-top">Group 52</a>
        <button class="navbar-toggler navbar-toggler-right" type="button"
        data-toggle="collapse" data-target="#navbarResponsive"
        aria-controls="navbarResponsive" aria-expanded="false"
        aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="navbarResponsive">
          <ul class="navbar-nav ml-auto">
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="./../../index.html">Home</a>
            </li>
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="./../problem_catagories/pamel.html">Problem Classes</a>
            </li>
            <li class="nav-item dropdown">
              <!-- a class="nav-link js-scroll-trigger" href="#">Models</a-->
              <a class="nav-link dropdown-toggle" id="navbarDropdownMenuLink"
              data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
              Models
              </a>
              <div class="dropdown-menu" aria-labelledby="navbarDropdownMenuLink">
              <a class="nav-link dropdown-item" href="./giovanni_2.html">MapReduce</a>
              <a class="nav-link dropdown-item" href="./../tensorflow/rajat.html">TensorFlow</a>
              <a class="nav-link dropdown-item" href="./../spark/rasika.html">Spark</a>
              </div>
            </li>
          </ul>
        </div>
      </div>
    </nav>

    <header class="masthead text-center text-white d-flex personal-section-header">
      <div class="container my-auto"> <!-- my-auto"-->
        <div class="row">
          <div class="col">
            <h1 class="text-uppercase personal-section-title">
              <strong>Title</strong>
            </h1>
          </div>
        </div>
        <div class="row">
          <div class="col">
            <p class="text-faded personal-section-author">Researched By:</p>
          </div>
        </div>
      </div>
    </header>

    <section class="personal-section-body">

    <div class="container-fluid">
    <div class="row">
    <nav class="col-md-3 d-none d-md-block navbar navbar-expand-lg navbar-light" id="sideMenu" style="background-color: transparent">
      <ul class="nav nav-pills flex-column" style="">
        <li class="nav-item">
          <a class="nav-link" href="#Introduction">Introduction</a>
          <hr class="sideMenuDivider">
        </li>
        <li class="nav-item">
          <a class="nav-link" href="#Programming Model">Programming Model</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="#Implementation">Implementation</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="#Execution Overview">Execution Overview</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="#Advantages">Advantages</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="#Disadvantages">Disadvantages</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="#Performance">Performance</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="#Refinements and Variations">Refinements and Variations</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="#Real-World Examples">Real-World Examples</a>
        </li>
      </ul>
    </nav>
    <div class="col-md-9 col-sm-12 scrollspy-example" data-spy="scroll" data-target="#sideMenu" align="justify">
      <div id="Introduction" class="">
        <article>
        <h1>Introduction</h1>
        <hr>
        <p>MapReduce is a programming model and associatied implementation firstly invented by Google to
          deal with indexing the exploding volume of the content of the web, and lately adopted by many
          other companies for processing and generating Big Data sets with a parallel, distributed algorithm
          on a cluster. Although most such computations are conceptually relatively straightforward, the
          input data is usually quite large and the computations have to be distributed across hundreds or
          thousands of machines in order to finish in a reasonable amount of time. Furthermore, the issue
          of how to parallelise the computation, distribute the data and handle failures flood the original
          simple computation with large amount of complex and structured code in order to deal with these issues.
          As a way to tackle this problems, a new abstraction was designed to isolate the application from
          the details of running a distributed program.</p>
        </article>
      </div>
     <div id="Programming Model" class="">
      <article>
        <h1>Programming Model</h1>
        <hr>
        <p>The MapReduce model is a specialisation of the split-apply-combine strategy for data analysis (Wickam, 2011). The
           name draws its inspiration from the map and reduce functions commonly used in functional
           programming language like Lisp.
        </p>
        <p>The computation takes a set of input key/value pairs of data with a type in one data domain and
          produces a set of output key/values pairs in a different domain. The user of the MapReduce library
          has to define the two functions at the core of its computation: Map and Reduce (Jeffrey Dean, Sanjay Ghemawat, 2004).
          <br />
          <ul>
            <li>Map takes an input pair and produces a set of intermediate key/values pairs. The
                MapReduce library groups together all intermediate values associated with the same
                intermediate key I and passes them to the Reduce function.</li>
            <li>The Reduce function accepts an intermediate key I and a set of values for that key. It
                merges together these values to form a possibly smaller set. Tipically just zero or one
                output value is produced from one Reduce invocation.</li>
            <br />
            The main advantage of this model is that it allows large computations to be easily parallelised and
            re-executed to be used as the primary mechanism for fault tolerance.
          </ul>
        </p>
      </article>
     </div>
      <div id="Example (WordCount)" class="">
        <h2>Example (WordCount)</h2>
        <p>Figure 1 illustrates an example MapReduce program expressed in pseudo-code for counting the
           number of occurences of each word in a collection of documents (Aris Gkoulalas, 2014).
        </p>
        <br />
         <center>
           <img alt="Figure 1 - MapReduce WordCount pseudo-code example" title="Figure 1 - MapReduce WordCount pseudo-code example" src="images/WordCount.png"</img>
           <br />
           <figcaption>Figure 1 - MapReduce WordCount pseudo-code example</figcaption>
           </img>
         </center>
        <br />
        <p>In this example, the map function emits each word plus an associated mark of occurences, while
           the reduce function sums together all marks emitted for a particular word.
        </p>
      </div>
     <div id="Implementation" class="">
        <article>
        <h1>Implementation</h1>
        <hr>
        <p>Many different implementation of the MapReduce interface are possible, each one tailored for a
           specific problem. In principle, the design of the MapReduce library considers the following main
           principles (Aris Gkoulalas, 2014):
        </p>
        <ul>
          <li>Low-Cost Unreliable Commodity Hardware: Instead of using expensive, high-performance,
              reliable symmetric multiprocessing (SMP) or massively parallel processing (MPP) machines
              equipped with high-end network and storage subsystems, the MapReduce framework is
              designed to run on large clusters of commodity hardware. This hardware is managed and
              powered by open-source operating systems and utilities so that the cost is kept low.
          </li>
          <li>Extremely Scalable RAIN Cluster: Instead of using centralized RAID-based SAN or NAS
              storage systems, every MapReduce node has its own local off-theshelf hard drives. These
              nodes are loosely coupled in rackable systems that are connected with generic LAN
              switches. These nodes can be taken out of service with almost no impact to still-running
              MapReduce jobs. These clusters are called Redundant Array of Independent (and
              Inexpensive) Nodes (RAIN).
          </li>
          <li>Fault-Tolerant yet Easy to Administer: MapReduce jobs can run on clusters with thousands
              of nodes or even more. These nodes are not very reliable as at any point in time, a certain
              percentage of these commodity nodes or hard drives will be out of order. Hence, the
              MapReduce framework applies straightforward mechanisms to replicate data and launch
              backup tasks so as to keep still-running processes going. To handle crashed nodes, system
              administrators simply take crashed hardware off-line. New nodes can be plugged in at any
              time without much administrative hassle. There is no complicated backup, restore and
              recovery configurations like the ones that can be seen in many DBMS.
          </li>
          <li>Highly Parallel yet Abstracted: The most important contribution of the MapReduce
              framework is its ability to automatically support the parallelization of task executions.
              Hence, it allows developers to focus mainly on the problem at hand rather than worrying
              about the low level implementation details, such as memory management, file allocation,
              parallel, multi-threaded or network programming. Moreover, MapReduce’s shared-nothing
              architecture [38] makes it much more scalable and ready for parallelization.
          </li>
        </ul>
        <br />
        <p>
          For the sake of argument, we will describe an implementation targeted to the computing
          enviroment used at Google: large clusters of commodity PCs connected together with switched Ethernet (Jeffrey Dean, Sanjay Ghemawat, 2004):
        </p>
        <ul>
          <li>Machines are typically dual-processor x86 processors running Linux, with 2-4 GB of
              memory per machine.
          </li>
          <li>Commodity networking hardware is used – typically either 100 megabits/second or 1
              gigabit/second at the machine level, but averaging considerably less in overall bisection
              bandwidth.
          </li>
          <li>A cluster consists of hundreds or thousands of machines, and therefore machine failures
              are common.
              Ethernet:
          </li>
          <li>Storage is provided by inexpensive IDE disks attached directly to individual machines. A
              distributed file system [8] developed in-house is used to manage the data stored on these
              disks. The file system uses replication to provide availability and reliability on top of
              unreliable hardware.
          </li>
          <li>Users submit jobs to a scheduling system. Each job consists of a set of tasks, and is mapped
              by the scheduler to a set of available machines within a cluster.
          </li>
        </ul>
      </article>
     </div>
      <div id="Execution Overview" class="">
        <article>
        <h1>Execution Overview</h1>
        <hr>
        <p>The MapReduce computations are compositions of two phases (Dimitrov, 2016):
          <br />
          <br />
          <i>MapPhase: list(k1, v1) -> list(k2, v2)</i>
          <br />
          <br />
          The framework accepts a list of key / value pairs on input. For each pair, it calls a map that
          generates a list of intermediate key / value pair.
          <br />
          <br />
          The ReductionPhase is subdivided in two subphases:
          <br />
          <br />
          <i>preparation: list(k2, v2) -> list(k2, list(v2))</i>
          <br />
          <br />
          In the preparation phase, the framework sorts the lists produced during the previous phase, and the
          result is a list of intermediate keys / list of values. In this list, each key is associated with all the
          values generated for it.
          <br />
          <br />
          <i>reduction: list(k2, list(v2)) -> list(k2, v2)</i>
          <br />
          <br />
          After that, the reduce phase. For every pair in the list, the framework calls the function reduce,
          which produces a list of values for the key.
        </p>
        <p>In the Google implementation, the Map calls are distributed across multiple machines by
           automatically partitioning the input data into a set of M splits. The input splits can be processed in
           parallel by different machines. Reduce invocations are distributed by partitioning the intermediate
           key space into R pieces using a partitioning function, usually a hash function (e.g. hash(key) mod
           R). However, the number of partitions (R) and the partitioning function are specified by the user.
           When the user program invokes the MapReduce function, the following sequence of actions is
           executed (Jeffrey Dean, Sanjay Ghemawat, 2004):
           <ul>
             <li>The MapReduce library in the user program first splits the input files into M pieces of
                 typically 16 megabytes to 64 megabytes (MB) per piece (controllable by the user via an
                 optional parameter). It then starts up many copies of the program on a cluster of machines
            </li>
            <br />
            <li>One of the copies of the program is special: the master. The rest are workers that are
                assigned work by the master. There are M map tasks and R reduce tasks to assign. The
                master picks idle workers and assigns each one a map task or a reduce task.
            </li>
            <br />
            <li>A worker who is assigned a map task reads the contents of the corresponding input split. It
                parses key/value pairs out of the input data and passes each pair to the user-defined Map
                function. The intermediate key/value pairs produced by the Map function are buffered in
                memory.
            </li>
            <br />
            <li>Periodically, the buffered pairs are written to local disk, partitioned into R regions by the
                partitioning function. The locations of these buffered pairs on the local disk are passed
                back to the master, who is responsible for forwarding these locations to the reduce
                workers.
            </li>
            <br />
            <li>When a reduce worker is notified by the master about these locations, it uses remote
                procedure calls to read the buffered data from the local disks of the map workers. When a
                reduce worker has read all intermediate data, it sorts it by the intermediate keys so that all
                occurrences of the same key are grouped together. The sorting is needed because typically
                many different keys map to the same reduce task. If the amount of intermediate data is
                too large to fit in memory, an external sort is used.
            </li>
            <br />
            <li>The reduce worker iterates over the sorted intermediate data and for each unique
                intermediate key encountered, it passes the key and the corresponding set of intermediate
                values to the user’s Reduce function. The output of the Reduce function is appended to a
                final output file for this reduce partition.
            </li>
            <br />
            <li>When all map tasks and reduce tasks have been completed, the master wakes up the user
                program. At this point, the MapReduce call in the user program returns back to the user
                code.
            </li>
           </ul>
        </p>
        <p>Figure 2 show the execution of a MapReduce operation in our implementation (Jeffrey Dean, Sanjay Ghemawat, 2004).
          <br />
             <center>
             <img alt="Figure 2 - MapReduce Execution" title="MapReduce Execution" src="images/MapReduce Execution.gif" width="483" height="340"
             <br />
             <figcaption>Figure 2 - MapReduce Execution</figcaption>
             </img>
           </center>
          <br />
          When the algorithm terminates, the output of the MapReduce execution is available in the R
          output files (one per each reduce task, with file names specified by the user). Typically, users do
          not merge these R output files as they are usually passed to another MapReduce call or used by
          another distributed application which is able to process input that is partitioned into multiple files.
          <br />
          <br />
          Figure 3 illustrates the execution of the wordcount example using MapReduce (Aris Gkoulalas, 2014).
          <br />
             <center>
             <img alt="Figure 3 - MapReduce WordCount Execution Example" title="MapReduce Execution" src="images/MapReduce WordCount Execution Example.png" width="687" height="260"
             <br />
             <figcaption>Figure 3 - MapReduce WordCount Execution Example</figcaption>
             </img>
           </center>
          <br />
        </p>
      </article>
      <div id="Advantages" class="">
          <article>
          <h1>Advantages</h1>
          <hr>
          <h4>Easy to use</h4>
          <p>One of the main advantages of the MapReduce model is that it can be easily used by programmers
            who have no experience with parallel and distributed systems (Jeffrey Dean, Sanjay Ghemawat, 2004). Also, the key contributions of the
            MapReduce framework are not the actual map and reduce functions, but the scalability and fault-tolerance
            achieved for a variaty of applications by optimising the execution engine. As a
            consequence, relative improvement can only be seen with multi-threaded implementations.
          </p>
          <h4>Fault-tolerance and Determinism</h4>
          <p>Since it was programmed to tackle massive amounts of data, the MapReduce model must tolerate
             machine failures peacefully (Jeffrey Dean, Sanjay Ghemawat, 2004). The master is responsible for scheduling the tasks and tackle workers
             failures. For each map task and reduce task, it stores the state (idle, in-progress, or completed),
             and the identity of the worker machine (for non-idle tasks). For each completed map task, the
             master stores the locations and sizes of the R intermediate file regions produce by the map.
             MapReduce is resilient to large-scale worker failure. The master pings every worker periodically
             and if it does not respond after a reasonable amount of time it is marked as failed. Any map tasks
             completed by a failed worker are simply re-executed in order to make progress. If the master fails,
             the job must be re-executed from scratch.
          </p>
          <p>Another important aspect of the MapReduce model is ist determinism (Dimitrov, 2016). Since map and reduce
             function may be deterministic regarding their inputs, the framework guarantees that the program
             will generate the same output each time. Furthermore, when user-defined functions are
             nondeterministic, the framework assures that for each distributed execution, there exist at least
             one sequential execution that can produce the same output.
          </p>
          <h4>Locality and Backup Tasks</h4>
          <p>Since network bandwidth is a relatively scarce resource in a distributed system the MapReduce
          model takes advantage of the fact that the input data is stored on the local disks of the machines
          that make up the cluster (Jeffrey Dean, Sanjay Ghemawat, 2004). The MapReduce master takes the location information of the input files
          into account and attempts to schedule a map task on a machine that contains a replica of that
          input’s task data, or near a replica of that task’s input data. This way, most input data is read
          locally and consumes no network bandwidth. Furthermore, when a MapReduce operation is close
          to completion, the master schedules backup executions of the remaining in-progress tasks. The
          task is marked as completed whenever either the primary or the backup execution completes, so
          that if a machine takes an unusual time to complete a task, it does not slow down the entire
          process.
        </p>
      </article>
      </div>
      <div id="Disadvantages" class="">
          <article>
          <h1>Disadvantages</h1>
          <hr>
          <p>One of the main disadvanteges of the MapReduce model is not being suitable for real time
             processing. MapReduce is a useful tool for batch processing, a process in which data is first
             collected over a period of time, then processed by a separated program and finally is retrieved.
             However, these steps usually require a reasonable amount of time, which is a tradeoff to their
             powerful performances. For this very reason, it is clear that a MapReduce solution would not be
             feasible to get data processed and retrieved in real time. Furthermore, every map task performed
             by each worker must be independent from the other, that is there must be no dependency
             between intermediate processes (job isolation).
          </p>
        </article>
      </div>
      <div id="Performance" class="">
          <article>
          <h1>Performance</h1>
          <hr>
          <p>As we said before, the MapReduce process requires a a long time to be completed and therefore is
             not suitable for fast response programs. However, there are several factors which affect the
             MapReduce process overall performance (Dawei Jiang, Beng Chin Ooi, Lei Shi, Sai Wu, 2010):
             <br />
             <ul>
               <li>Despite being independent of the underlying storage system, it still requires the storage
                   system to provide the I/O interface for scanning data.
               </li>
               <br />
               <li>Like existing database systems, it can actually exploit index to improve the performance of
                   data processing, improving by a factor of 2.5 up to 10 depending on the task to be
                   executed. In particular, MapReduce can efficiently process three kinds of index structures:
                   sorted files with range-index, B+-tree files and database indexed tables.
               </li>
               <br />
               <li>Although the MapReduce model only requires the user to define to function in order to
                   work properly (map and reduce) it does not specify how intermediate data produced by
                   map functions are grouped for reduce function to process. More advanced sort-merge
                   grouping algorithms could therefore introduce a considerable improvement in
                   performance.
               </li>
             </ul>
             <br />
             However, MapReduce programs are not guaranteed to be fast, as their main benefit resides in
             exploiting the grouping operations of the platform and its simple interface which require only to
             write the map and reduce functions.
          </p>
        </article>
      </div>
      <div id="Refinements and Variations" class="">
          <article>
          <h1>Refinements and Variations</h1>
          <hr>
          <p>Although the basic functionality provided by MapReduce is sufficient for most needs, there are a
             few extension which could be useful (Jeffrey Dean, Sanjay Ghemawat, 2004):
          </p>
          <h4>Partitioning Function</h4>
          <p>Despite data being partitioned across the tasks using a default partitioning function (usually a
             hashing function) this variation allows the user to define a custom partitioning function, as well as
             map and reduce functions, to split data according to specific needs. For example, sometimes the
             output keys are URLs, and we want all entries for a single host to end up in the same output file.
          </p>
          <h4>Combiner Function</h4>
          <p>In some cases, there is significant repetition in the intermediate keys produced by each map task
             (like in the wordcount example). An interesting variation is used to allow the user to specify an
             optional Combiner function that does partial merging of this data before it is sent over the
             network. Typically the same code is used to implement both the combiner and the reduce
             functions. Furthermore, it significantly speeds up certain classes of MapReduce operations.
          </p>
          <h4>Map-Reduce-Merge</h4>
          <p>One of the main limitation of the MapReduce framework is that it does not provide any method to
             join multiple datasets in one task. However, this could still be achieved by introducing an
             additional MapReduce step. In this new model, the map function transforms an input key/value
             pair (k1, v1) into a list of intermediate key/value pairs [(k2, v2)] (Hung-chih Yang, Ali Dasdan, Ruey-Lung Hsiao, D. Stott Parker, 2007). The reduce function aggregates
             the list of values [v2] associated with k2 and produces a list of values [v3], which is also associated
             with k2. Note that inputs and outputs of both functions belong to the same lineage, say α. Another
             pair of map and reduce functions produce the intermediate output (k3, [v4]) from another lineage,
             say β. Based on keys k2 and k3, the merge function combines the two reduced outputs from
             different lineages into a list of key/value outputs [(k4, v5)]. This final output becomes a new
            lineage, say γ. If α = β, then this merge function does a self-merge, similar to self-join in relational
            algebra. This could be useful for relational data processing on large clusters.
          </p>
        </article>
      </div>
      <div id="Real-World Examples" class="">
          <h1>Real-World Examples</h1>
          <hr>
          <p>Despite being created by Google, the MapReduce model has been adopted by many other
             companies. Nowadays, it has been used across a wide range of domains such as (P. Sudha, Dr. R. Gunavathi, 2016):
             <ul>
               <li>Large-scale machine learning problems</li>
               <br />
               <li>clustering problems for the Google News and Froogle products</li>
               <br />
               <li>extracting data to produce reports of popular queries(e.g. Google Zeitgeist and Google
                   Trends)</li>
               <br />
               <li>extracting properties of web pages for new experiments and products (e.g. extraction of
                  geographical locations from a large corpus of Web Pages for localized search)</li>
               <br />
               <li>processing of satellite imagery data</li>
               <br />
               <li>Language model processing for statistical machine Translation</li>
               <br />
               <li>Large-scale graph computations</li>
             </ul>
             <br />
             One of the most widespread and used implementation is Apache Hadoop.
             Further information regarding practical examples can be found here.
          </p>
      </div>
      <br />
      <div id="References" class="">
          <h5>References</h5>
          <i>Wickham, Hadley (2011). "The split-apply-combine strategy for data analysis". Journal of Statistical Software</i>
          <br />
          <br />
          <i>MapReduce: Simplified Data Processing on Large Clusters, by Jeffrey Dean and Sanjay Ghemawat; from Google
            Research</i>
          <br />
          <br />
          <i>Aris Gkoulalas-Divanis, Large-Scale Data Analytics, Abderrahim Labbi Editors</i>
          <br />
          <br />
          <i>The Performance of MapReduce: An In-depth Study, Dawei Jiang, Beng Chin Ooi, Lei Shi, Sai Wu, School of
             Computing National University of Singapore</i>
          <br />
          <br />
          <i>Encyclopedia of Cloud Computing, Vladimir Dimitrov, University of Sofia, Bulgaria</i>
          <br />
          <br />
          <i>A Survey Paper on Map Reduce in Big Data, P. Sudha, Dr. R. Gunavathi, Sree Saraswathi Thyagaraja College</i>
          <br />
          <br />
          <i>Map-Reduce-Merge: Simplified Relational Data Processing on Large Clusters, Hung-chih Yang, Ali Dasdan,
             Yahoo!, Ruey-Lung Hsiao, D. Stott Parker, Computer Science Department, UCLA</i>
          <br />
      </div>
      </div>
    </div>
  </div>
</div>
</section>

    <section class="" id="footer" style="background-color:#F38945; color:rgba(255, 255, 255, 0.7); box-shadow: 0px 2px 10px 5px rgba(100, 100, 100, 0.49);">
      <div class="container-fluid">
        <div class="row">
          <div class="col-sm">
            <p class="" style="color: #fff">Contributors</p>
            <hr style="border-top: 1px solid #4B4135; border-width: 3px;">
            <div class="footer-text">
            	Rajat Rasal <br>
            	Rasika Navarange <br>
            	Giovanni Caruso <br>
            	Pamelpreet Jhinger <br>
            </div>
          </div>
          <div class="col-sm">
            <p class="" style="color: #fff">Templates</p>
            <hr style="border-top: 1px solid #4B4135; border-width: 3px;">
            <div class="footer-text">
              <a href="https://github.com/BlackrockDigital/startbootstrap-creative"
              style="color:rgba(255, 255, 255, 0.7);">
              <i class="fab fa-2x fa-github"></i><br>
              startbootstrap-creative
              </a><br>
              altered by Rajat Rasal
            <div>
          </div>
        </div>
      </div>
    </section>


    <script src="../../vendor/jquery/jquery.min.js"></script>
    <script src="../../vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <!--link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script-->

    <!-- Plugin JavaScript -->
    <script src="../../vendor/jquery-easing/jquery.easing.min.js"></script>
    <script src="../../vendor/scrollreveal/scrollreveal.min.js"></script>
    <script src="../../vendor/magnific-popup/jquery.magnific-popup.min.js"></script>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js"></script>

    <!-- Custom scripts for this template -->
    <!-- script src="js/creative.min.js"></script-->
    <script src="../../js/creative.js"></script>
    <script src="../../js/personal_page_scripts.js"></script>

  </body>

</html>
